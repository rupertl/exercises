#+TITLE: Introduction to Statistical Learning, Chapter 5: Resampling Methods
#+AUTHOR: Rupert Lane
#+EMAIL: rupert@rupert-lane.org
#+PROPERTY: header-args:R :session *R*
#+STARTUP: inlineimages
#+STARTUP: latexpreview

* Conceptual
#+BEGIN_SRC R :exports code :results none
  library(tidyverse)
  library(ggplot2)
  library(ISLR)

  options(crayon.enabled = FALSE)
#+END_SRC

** Question 1
#+BEGIN_QUOTE
Using basic statistical properties of the variance, as well as single-
variable calculus, derive (5.6). In other words, prove that /α/ given
by (5.6) does indeed minimize Var(/αX/ + (1 − /α/) /Y/).
#+END_QUOTE

First we need to turn the above equation into a function of /α/. using
some of the [[https://en.wikipedia.org/wiki/Variance#Basic_properties][basic properties]] of the variance:

#+BEGIN_SRC latex :exports results :results raw  :file img/ch05q01_1.png
$$\mathrm{Var}(\alpha X + (1-\alpha)Y)$$
$$= \alpha^2\mathrm{Var}(X) + (1-\alpha)^{2}\mathrm{Var}(Y) + 2\alpha(1-\alpha)\mathrm{Cov}(X, Y)$$
$$= \alpha^2\mathrm{Var}(X) + (1-\alpha)^{2}\mathrm{Var}(Y) + 2\alpha(1-\alpha)\mathrm{Cov}(X, Y)$$
#+END_SRC

#+RESULTS:
[[file:img/ch05q01_1.png]]

To find the minimum, solve /f'(α)/ = 0:

#+BEGIN_SRC latex :exports results :results raw  :file img/ch05q01_2.png
$$0 = 2\alpha\mathrm{Var}(X) + 2(\alpha - 1)\mathrm{Var}(Y) + 2(1 - 2\alpha)\mathrm{Cov}(X, Y)$$
$$2(\mathrm{Var}(Y) - \mathrm{Cov}(X,Y)) = 2\alpha\mathrm{Var}(X) + 2\alpha\mathrm{Var}(Y) - 4\alpha\mathrm{Cov}(X, Y)$$
$$\alpha = \frac{2(\mathrm{Var}(Y) - \mathrm{Cov}(X,Y))}{2\mathrm{Var}(X) + 2\mathrm{Var}(Y) - 4\mathrm{Cov}(X, Y)}$$
$$\alpha = \frac{\mathrm{Var}(Y) - \mathrm{Cov}(X,Y)}{\mathrm{Var}(X) + \mathrm{Var}(Y) - 2\mathrm{Cov}(X, Y)}$$
$$\alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 -2\sigma_{XY}}$$
#+END_SRC

#+RESULTS:
[[file:img/ch05q01_2.png]]

** Question 2
#+BEGIN_QUOTE
We will now derive the probability that a given observation is part of
a bootstrap sample. Suppose that we obtain a bootstrap sample from a
set of /n/ observations.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
What is the probability that the first bootstrap observation is /not/
the /j/-th observation from the original sample? Justify your answer.
#+END_QUOTE

There are /n/ possible outcomes, and we will only draw the /j/-th
observation one of these times, so the probability is /n-1/ ÷ /n/
*** (b) 
#+BEGIN_QUOTE
What is the probability that the second bootstrap observation is /not/
the /j/-th observation from the original sample?
#+END_QUOTE

The same, as we draw with replacements each time.
*** (c) 
#+BEGIN_QUOTE
Argue that the probability that the /j/-th observation is not in the
bootstrap sample is /(1 − 1 / n)^n/.
#+END_QUOTE

Assuming we draw /n/ samples, and as we have shown that each draw is
independent of each other in (b), we multiply the probability in (a) by
itself /n/ times, which is equivalent to /(1 − 1 / n)^n/.
*** (d) 
#+BEGIN_QUOTE
When /n/ = 5, what is the probability that the /j/-th observation is
in the bootstrap sample?
#+END_QUOTE

From (c), the probability of the /j/-th observation not appearing is
/(4/5)^5/ or 0.328, so the probability it appears is 1 - 0.328 or 0.672.
*** (e) 
#+BEGIN_QUOTE
When /n/ = 100, what is the probability that the /j/-th observation is in
the bootstrap sample?
#+END_QUOTE

Using the same equation, 0.634.
*** (f) 
#+BEGIN_QUOTE
When /n/ = 10,000, what is the probability that the /j/-th observation
is in the bootstrap sample?
#+END_QUOTE

Using the same equation, 0.632.
*** (g) 
#+BEGIN_QUOTE
Create a plot that displays, for each integer value of /n/ from 1 to
100,000, the probability that the /j/-th observation is in the
bootstrap sample. Comment on what you observe.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics  :file img/ch05q02g_1.png :width 800
  prob <- function(n) { return(1 - (1 - (1 / n))^n) }
  data_frame(n=1:100000, p=prob(n)) %>%
    ggplot(aes(x=n, y=p)) +
    geom_point() +
    labs(title = "Plot of probability of j-th observation appearing for bootstrap of size n")
#+END_SRC

#+RESULTS:
[[file:img/ch05q02g_1.png]]

The probability starts at 1 (for a single observation, it must appear)
and converges quickly om around 0.632.
*** (h) 
#+BEGIN_QUOTE
We will now investigate numerically the probability that a bootstrap
sample of size /n/ = 100 contains the /j/-th observation. Here /j/ = 4. We
repeatedly create bootstrap samples, and each time we record whether
or not the fourth observation is contained in the bootstrap sample.

Comment on the results obtained.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  store = rep(NA, 10000)
  for (i in 1:10000) {
    store[i]= sum(sample(1:100, rep=TRUE) == 4) > 0
  }
  mean(store)
#+END_SRC

#+RESULTS:
: 
: [1] 0.6361

This is close to the result we got in (e), but we will get slightly
different results each time as the samples change
** Question 3
#+BEGIN_QUOTE
We now review /k/-fold cross-validation.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Explain how /k/-fold cross-validation is implemented.
#+END_QUOTE

Divide the training set into /k/ pieces. Train the model on all but
one of the pieces (the hold-out piece) and then calculate the MSE
using the hold out piece as test data. Do the train-test cycle again
for all other possible hold-out pieces. You can then find the
cross-validation error by summing each of the MSEs and dividing by /k.
*** (b) 
#+BEGIN_QUOTE
What are the advantages and disadvantages of /k/-fold cross-validation
relative to:

i. The validation set approach?

ii. LOOCV?
#+END_QUOTE

Compared to the validation set approach, /k/-fold allows us to get a
better estimate of test error as we get to use more data, and can
mitigate the issue of one set having outliers. However, we do need to
do more computation as we train the model multiple times.

LOOCV will have less bias, as we use /n/-1 items of the data set, but
more variance as there is only one test point. LOOCV is even more
computationally expensive than /k/-fold.
** Question 4
#+BEGIN_QUOTE
Suppose that we use some statistical learning method to make a
prediction for the response /Y/ for a particular value of the
predictor /X/. Carefully describe how we might estimate the standard
deviation of our prediction.
#+END_QUOTE

We can use the bootstrap, running a model multiple times by drawing
data from the population with replacement. The s.d. of the set of
predictors generated by the models is then an estimate of the s.d. of
the predictor.
* Applied
#+BEGIN_SRC R :exports code :results none
  library(tidyverse)
  library(ggplot2)
  library(ISLR)
  library(GGally)
  library(boot)
  library(MASS)

  options(crayon.enabled = FALSE)
#+END_SRC

** Question 5
#+BEGIN_QUOTE
In Chapter 4, we used logistic regression to predict the probability
of default using ~income~ and ~balance~ on the ~Default~ data set. We
will now estimate the test error of this logistic regression model
using the validation set approach. Do not forget to set a random seed
before beginning your analysis.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Fit a logistic regression model that uses ~income~ and ~balance~ to
predict ~default~.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  set.seed(1)
  default <- as_tibble(Default)
  lrModel <- glm(default ~ income + balance,
                 data = default, family = binomial)
  summary(lrModel)
#+END_SRC 

#+RESULTS:
#+begin_example

Call:
glm(formula = default ~ income + balance, family = binomial, 
    data = default)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4725  -0.1444  -0.0574  -0.0211   3.7245  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***
income       2.081e-05  4.985e-06   4.174 2.99e-05 ***
balance      5.647e-03  2.274e-04  24.836  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2920.6  on 9999  degrees of freedom
Residual deviance: 1579.0  on 9997  degrees of freedom
AIC: 1585

Number of Fisher Scoring iterations: 8
#+end_example

*** (b) 
#+BEGIN_QUOTE
Using the validation set approach, estimate the test error of this
model. In order to do this, you must perform the following steps:

i. Split the sample set into a training set and a validation set.

ii. Fit a multiple logistic regression model using only the training
observations.

iii. Obtain a prediction of default status for each individual in the
validation set by computing the posterior probability of default for
that individual, and classifying the individual to the default
category if the posterior probability is greater than 0.5.

iv. Compute the validation set error, which is the fraction of the
observations in the validation set that are misclassified.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  default$id <- 1:nrow(default)

  ## Take two vectors and calculate the rate where they don't agree
  calcErrorRate <- function(predict, actual) {
    results <- table(predict, actual)
    print(results)
    errorRate <- (results[1,2] + results[2,1]) /
      (results[1,1] + results[2,2] + results[1,2] + results[2,1])
    print(paste("Error rate:", errorRate * 100, "%"))
  }

  ## Try the validation set approach
  tryValidation <- function(default) {
    defaultTrain <- default %>% sample_frac(0.75)
    defaultValidation <- anti_join(default, defaultTrain, by='id')
    defaultModel <- glm(default ~ income + balance,
                        data = defaultTrain, family = binomial)
    defaultPredict <- data_frame(prob=predict(defaultModel, defaultValidation,
                                              type="response"),
                                 predict=ifelse(prob > 0.5, 1, 0),
                                 actual=defaultValidation$default)
    calcErrorRate(defaultPredict$predict, defaultPredict$actual)
  }

  tryValidation(default)
#+END_SRC 

#+RESULTS:
: 
:        actual
: predict   No  Yes
:       0 2413   56
:       1    9   22
: [1] "Error rate: 2.6 %"

*** (c) 
#+BEGIN_QUOTE
Repeat the process in (b) three times, using three different splits of
the observations into a training set and a validation set. Comment on
the results obtained.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  tryValidation(default)
  tryValidation(default)
  tryValidation(default)
#+END_SRC 

#+RESULTS:
#+begin_example
       actual
predict   No  Yes
      0 2413   52
      1    6   29
[1] "Error rate: 2.32 %"

       actual
predict   No  Yes
      0 2405   61
      1    4   30
[1] "Error rate: 2.6 %"

       actual
predict   No  Yes
      0 2390   79
      1    8   23
[1] "Error rate: 3.48 %"
#+end_example

So the test error is around 2.6% and varies depending on the samples
we take.

*** (d) 
#+BEGIN_QUOTE
Now consider a logistic regression model that predicts the probability
of ~default~ using ~income~, ~balance~, and a dummy variable for ~student~.
Estimate the test error for this model using the validation set
approach. Comment on whether or not including a dummy variable for
student leads to a reduction in the test error rate
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  ## Try the validation set approach with a student variable
  tryValidationStudent <- function(default) {
    defaultTrain <- default %>% sample_frac(0.75)
    defaultValidation <- anti_join(default, defaultTrain, by='id')
    defaultModel <- glm(default ~ income + balance + student,
                        data = defaultTrain, family = binomial)
    defaultPredict <- data_frame(prob=predict(defaultModel, defaultValidation,
                                              type="response"),
                                 predict=ifelse(prob > 0.5, 1, 0),
                                 actual=defaultValidation$default)
    calcErrorRate(defaultPredict$predict, defaultPredict$actual)
  }

  tryValidationStudent(default)
  tryValidationStudent(default)
  tryValidationStudent(default)
#+END_SRC 

#+RESULTS:
#+begin_example

       actual
predict   No  Yes
      0 2412   52
      1   16   20
[1] "Error rate: 2.72 %"

       actual
predict   No  Yes
      0 2403   58
      1    8   31
[1] "Error rate: 2.64 %"

       actual
predict   No  Yes
      0 2403   60
      1   13   24
[1] "Error rate: 2.92 %"
#+end_example

Based on these results we don't see a big difference by adding
~student~.
** Question 6
#+BEGIN_QUOTE
We continue to consider the use of a logistic regression model to
predict the probability of ~default~ using ~income~ and ~balance~ on
the ~Default~ data set. In particular, we will now compute estimates
for the standard errors of the ~income~ and ~balance~ logistic
regression coefficients in two different ways: (1) using the
bootstrap, and (2) using the standard formula for computing the
standard errors in the ~glm()~ function. Do not forget to set a random
seed before beginning your analysis.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Using the summary() and glm() functions, determine the estimated
standard errors for the coefficients associated with ~income~ and
~balance~ in a multiple logistic regression model that uses both
predictors.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  summary(lrModel)
#+END_SRC 

#+RESULTS:
#+begin_example

Call:
glm(formula = default ~ income + balance, family = binomial, 
    data = default)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4725  -0.1444  -0.0574  -0.0211   3.7245  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***
income       2.081e-05  4.985e-06   4.174 2.99e-05 ***
balance      5.647e-03  2.274e-04  24.836  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2920.6  on 9999  degrees of freedom
Residual deviance: 1579.0  on 9997  degrees of freedom
AIC: 1585

Number of Fisher Scoring iterations: 8
#+end_example

The standard errors are around 4.5e-6 for ~income~ and 2.3e-4 for ~balance~.
*** (b) 
#+BEGIN_QUOTE
Write a function, ~boot.fn()~ , that takes as input the ~Default~ data
set as well as an index of the observations, and that outputs the
coefficient estimates for ~income~ and ~balance~ in the multiple
logistic regression model.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boot.fn <- function(default, obs) {
    return(coef(glm(default ~ income + balance,
                    data = default, 
                    family = binomial,
                    subset = obs)))
  }

  ## Test the function with the head of the data
  boot.fn(default, 1:1000) 
#+END_SRC 

#+RESULTS:
: 
:   (Intercept)        income       balance 
: -1.155359e+01  3.074546e-05  5.609161e-03

*** (c) 
#+BEGIN_QUOTE
Use the ~boot()~ function together with your ~boot.fn()~ function to
estimate the standard errors of the logistic regression coefficients
for ~income~ and ~balance~.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  ## This takes around 20s to run
  boot(default, boot.fn, 1000)
#+END_SRC 

#+RESULTS:
#+begin_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = default, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
         original        bias     std. error
t1* -1.154047e+01 -8.214332e-03 4.254393e-01
t2*  2.080898e-05  4.737281e-08 4.600125e-06
t3*  5.647103e-03  2.426216e-06 2.269350e-04
#+end_example

The standard errors from the bootstrap are around 4.7e-6 for ~income~
and 2.4e-4 for ~balance~.

*** (d) 
#+BEGIN_QUOTE
Comment on the estimated standard errors obtained using the ~glm()~
function and using your bootstrap function.
#+END_QUOTE

The bootstrap and ~glm()~ results are very similar.

** Question 7
#+BEGIN_QUOTE
In Sections 5.3.2 and 5.3.3, we saw that the ~cv.glm()~ function can
be used in order to compute the LOOCV test error estimate.
Alternatively, one could compute those quantities using just the
~glm()~ and ~predict.glm()~ functions, and a for loop. You will now
take this approach in order to compute the LOOCV error for a simple
logistic regression model on the ~Weekly~ data set. Recall that in the
context of classification problems, the LOOCV error is given in (5.4).
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Fit a logistic regression model that predicts ~Direction~ using ~Lag1~
and ~Lag2~.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  weekly <- as_tibble(Weekly)
  lrModel <- glm(Direction ~ Lag1 + Lag2,
                 data = weekly, family = binomial)
  summary(lrModel)
#+END_SRC 

#+RESULTS:
#+begin_example

Call:
glm(formula = Direction ~ Lag1 + Lag2, family = binomial, data = weekly)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.623  -1.261   1.001   1.083   1.506  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.22122    0.06147   3.599 0.000319 ***
Lag1        -0.03872    0.02622  -1.477 0.139672    
Lag2         0.06025    0.02655   2.270 0.023232 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1496.2  on 1088  degrees of freedom
Residual deviance: 1488.2  on 1086  degrees of freedom
AIC: 1494.2

Number of Fisher Scoring iterations: 4
#+end_example

*** (b) 
#+BEGIN_QUOTE
Fit a logistic regression model that predicts ~Direction~ using ~Lag1~
and ~Lag2~ using all but the first observation.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  lrModelAllButFirst <- glm(Direction ~ Lag1 + Lag2,
                            data = weekly[-1,], family = binomial)
  summary(lrModelAllButFirst)
#+END_SRC 

#+RESULTS:
#+begin_example

Call:
glm(formula = Direction ~ Lag1 + Lag2, family = binomial, data = weekly[-1, 
    ])

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6258  -1.2617   0.9999   1.0819   1.5071  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.22324    0.06150   3.630 0.000283 ***
Lag1        -0.03843    0.02622  -1.466 0.142683    
Lag2         0.06085    0.02656   2.291 0.021971 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1494.6  on 1087  degrees of freedom
Residual deviance: 1486.5  on 1085  degrees of freedom
AIC: 1492.5

Number of Fisher Scoring iterations: 4
#+end_example

*** (c) 
#+BEGIN_QUOTE
Use the model from (b) to predict the direction of the first
observation. You can do this by predicting that the first observation
will go up if P(~Direction~ = "Up" | ~Lag1~, ~Lag2~) > 0.5. Was this
observation correctly classified?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  predict.glm(lrModelAllButFirst, weekly[1,], type="response")  
  weekly[1,]
#+END_SRC 

#+RESULTS:
:         1 
: 0.5713923
: 
: # A tibble: 1 x 9
:    Year  Lag1  Lag2  Lag3   Lag4  Lag5 Volume Today Direction
:   <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl> <fct>    
: 1  1990 0.816  1.57 -3.94 -0.229 -3.48  0.155 -0.27 Down

So we predicted Up (P > 0.5) but the actual data was Down.

*** (d) 
#+BEGIN_QUOTE
Write a for loop from /i/ = 1 to /i/ = /n/, where /n/ is the number of
observations in the data set, that performs each of the following
steps:

i. Fit a logistic regression model using all but the /i/-th
observation to predict ~Direction~ using ~Lag1~ and ~Lag2~ .

ii. Compute the posterior probability of the market moving up for the
/i/-th observation.

iii. Use the posterior probability for the /i/-th observation in order
to predict whether or not the market moves up.

iv. Determine whether or not an error was made in predicting the
direction for the /i/-th observation. If an error was made, then
indicate this as a 1, and otherwise indicate it as a 0.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  error <- rep(0, nrow(weekly))
  for (i in 1:nrow(weekly)) {
    model <- glm(Direction ~ Lag1 + Lag2, data = weekly[-i,], family = binomial)
    prediction <- ifelse(predict.glm(lrModelAllButFirst,
                                     weekly[i,], type="response") > 0.5, "Up", "Down")
    error[i] <- ifelse(prediction != weekly[i, "Direction"], 1, 0)
  }
  head(error)
#+END_SRC 

#+RESULTS:
: 
: [1] 1 1 0 1 0 1

*** (e) 
#+BEGIN_QUOTE
Take the average of the /n/ numbers obtained in (d) iv in order to
obtain the LOOCV estimate for the test error. Comment on the results.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  mean(error)
#+END_SRC 

#+RESULTS:
: [1] 0.4435262

The error rate is around 44.4%. As the model coefficient p-values in
(a) are large, we'd expect a high error rate.
** Question 8
#+BEGIN_QUOTE
We will now perform cross-validation on a simulated data set.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Generate a simulated data set as follows:
#+END_QUOTE

#+BEGIN_SRC R :exports code :results none
  set.seed(1)
  x = rnorm(100)
  y = x - 2*x^2 + rnorm(100)
#+END_SRC

#+BEGIN_QUOTE
In this data set, what is /n/ and what is /p/? Write out the model
used to generate the data in equation form.
#+END_QUOTE

The population size /n/ is 100 and /p/ is two due to the relationship
between /y/ and both /x/ and /x²/.

As an equation, /y = x - 2x² + ε/.
*** (b) 
#+BEGIN_QUOTE
Create a scatterplot of /X/ against /Y/ . Comment on what you find.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics :file img/ch05q08b.png
  simData <- data_frame(x=x, y=y)
  ggplot(simData, aes(x=x, y=y)) + 
    geom_point() +
    labs(title = "Simulated data scatterplot")
#+END_SRC

#+RESULTS:
[[file:img/ch05q08b.png]]

We see a curve approximating the quadratic equation in (a).
*** (c) 
#+BEGIN_QUOTE
Set a random seed, and then compute the LOOCV errors that
result from fitting the following four models using least squares:

i. /Y = β₀ + β₁X + ε/

ii. /Y = β₀ + β₁X + β₂X² + ε/

iii. /Y = β₀ + β₁X + β₂X² + β₃X³ + ε/

iv. /Y = β₀ + β₁X + β₂X² + β₃X³ + β₄X⁴ + ε/

Note you may find it helpful to use the ~data.frame()~ function to
create a single data set containing both /X/ and /Y/ .
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  tryModels <- function(data) {
    errors <- rep(0, 4)
    for (i in 1:4) {
      lrModel <- glm(y~poly(x, i), data=data)
      errors[i] <- cv.glm(data, lrModel)$delta[1]
    }
    return(errors)
  }

  set.seed(1)
  tryModels(simData)
#+END_SRC 

#+RESULTS:
: 
: [1] 7.2881616 0.9374236 0.9566218 0.9539049

*** (d) 
#+BEGIN_QUOTE
Repeat (c) using another random seed, and report your results. Are
your results the same as what you got in (c)? Why?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  set.seed(42)
  tryModels(simData)
#+END_SRC 

#+RESULTS:
: 
: [1] 7.2881616 0.9374236 0.9566218 0.9539049

They are exactly the same, as LOOCV uses the same data each time -
there is no split into train and test.

*** (e)
#+BEGIN_QUOTE
Which of the models in (c) had the smallest LOOCV error? Is this what
you expected? Explain your answer.
#+END_QUOTE

The second model has the smallest error - this is expected as this is
the quadratic model.
*** (f) 
#+BEGIN_QUOTE
Comment on the statistical significance of the coefficient estimates
that results from fitting each of the models in (c) using least
squares. Do these results agree with the conclusions drawn based on
the cross-validation results?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  for (i in 1:4) {
    print(summary(glm(y~poly(x, i), data=simData)))
  }
#+END_SRC 

#+RESULTS:
#+begin_example


Call:
glm(formula = y ~ poly(x, i), data = simData)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-9.5161  -0.6800   0.6812   1.5491   3.8183  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -1.550      0.260  -5.961 3.95e-08 ***
poly(x, i)     6.189      2.600   2.380   0.0192 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 6.760719)

    Null deviance: 700.85  on 99  degrees of freedom
Residual deviance: 662.55  on 98  degrees of freedom
AIC: 478.88

Number of Fisher Scoring iterations: 2


Call:
glm(formula = y ~ poly(x, i), data = simData)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9650  -0.6254  -0.1288   0.5803   2.2700  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.5500     0.0958  -16.18  < 2e-16 ***
poly(x, i)1   6.1888     0.9580    6.46 4.18e-09 ***
poly(x, i)2 -23.9483     0.9580  -25.00  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 0.9178258)

    Null deviance: 700.852  on 99  degrees of freedom
Residual deviance:  89.029  on 97  degrees of freedom
AIC: 280.17

Number of Fisher Scoring iterations: 2


Call:
glm(formula = y ~ poly(x, i), data = simData)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9765  -0.6302  -0.1227   0.5545   2.2843  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.55002    0.09626 -16.102  < 2e-16 ***
poly(x, i)1   6.18883    0.96263   6.429 4.97e-09 ***
poly(x, i)2 -23.94830    0.96263 -24.878  < 2e-16 ***
poly(x, i)3   0.26411    0.96263   0.274    0.784    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 0.9266599)

    Null deviance: 700.852  on 99  degrees of freedom
Residual deviance:  88.959  on 96  degrees of freedom
AIC: 282.09

Number of Fisher Scoring iterations: 2


Call:
glm(formula = y ~ poly(x, i), data = simData)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0550  -0.6212  -0.1567   0.5952   2.2267  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.55002    0.09591 -16.162  < 2e-16 ***
poly(x, i)1   6.18883    0.95905   6.453 4.59e-09 ***
poly(x, i)2 -23.94830    0.95905 -24.971  < 2e-16 ***
poly(x, i)3   0.26411    0.95905   0.275    0.784    
poly(x, i)4   1.25710    0.95905   1.311    0.193    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 0.9197797)

    Null deviance: 700.852  on 99  degrees of freedom
Residual deviance:  87.379  on 95  degrees of freedom
AIC: 282.3

Number of Fisher Scoring iterations: 2
#+end_example

Yes, based on the p-values for the intercept, /x/ and /x²/ terms.
** Question 9
#+BEGIN_QUOTE
We will now consider the ~Boston~ housing data set, from the MASS
library.
#+END_QUOTE
*** (a) 
#+BEGIN_QUOTE
Based on this data set, provide an estimate for the population mean of
~medv~. Call this estimate /μ̂/.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boston <- as_tibble(Boston)
  mean(boston$medv)
#+END_SRC 

#+RESULTS:
: 
: [1] 22.53281

*** (b) 
Provide an estimate of the standard error of /μ̂/. Interpret this
result.

/Hint: We can compute the standard error of the sample mean by dividing the
sample standard deviation by the square root of the number of observations./

#+BEGIN_SRC R :results output :exports both
  sd(boston$medv) / sqrt(nrow(boston))
#+END_SRC 

#+RESULTS:
: [1] 0.4088611

The standard error of the sample mean is the standard deviation of the
variables divided by approximating 22.5.
*** (c) 
#+BEGIN_QUOTE
Now estimate the standard error of /μ̂/ using the bootstrap. How does
this compare to your answer from (b)?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boot.fn <- function(data, index) {
    return(mean(data$medv[index]))
  }
  boot(boston, boot.fn, R=1000)
#+END_SRC 

#+RESULTS:
#+begin_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = boston, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
    original       bias    std. error
t1* 22.53281 -0.003573123    0.408839
#+end_example

The results are similar to the third decimal place.
*** (d) 
#+BEGIN_QUOTE
Based on your bootstrap estimate from (c), provide a 95% confidence
interval for the mean of ~medv~ . Compare it to the results obtained
using ~t.test(Boston$medv)~. 

/Hint: You can approximate a 95% confidence interval using the
formula [μ̂ − 2SE(μ̂), μ̂ + 2SE(μ̂)]./
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  22.532 - (2 * 0.404)
  22.532 + (2 * 0.404)
  t.test(Boston$medv)
#+END_SRC 

#+RESULTS:
#+begin_example
[1] 21.724

[1] 23.34

	One Sample t-test

data:  Boston$medv
t = 55.111, df = 505, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 21.72953 23.33608
sample estimates:
mean of x 
 22.53281
#+end_example
*** (e) 
#+BEGIN_QUOTE
Based on this data set, provide an estimate, /μ̂_med/, for the median
value of ~medv~ in the population.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  median(boston$medv)
#+END_SRC 

#+RESULTS:
: [1] 21.2

*** (f) 
#+BEGIN_QUOTE
We now would like to estimate the standard error of /μ̂_med/ .
Unfortunately, there is no simple formula for computing the standard
error of the median. Instead, estimate the standard error of the
median using the bootstrap. Comment on your findings.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boot.fn <- function(data, index) {
    return(median(data$medv[index]))
  }
  boot(boston, boot.fn, R=1000)
#+END_SRC 

#+RESULTS:
#+begin_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = boston, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
    original   bias    std. error
t1*     21.2 -0.00715   0.3761487
#+end_example

The results are the same with a small standard error.
*** (g) 
#+BEGIN_QUOTE
Based on this data set, provide an estimate for the tenth percentile
of ~medv~ in Boston suburbs. Call this quantity /μ̂_0.1/ . (You can use
the ~quantile()~ function.)
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  quantile(boston$medv, c(0.1))
#+END_SRC 

#+RESULTS:
:   10% 
: 12.75

*** (h) 
#+BEGIN_QUOTE
Use the bootstrap to estimate the standard error of /μ̂_0.1/.1 .
Comment on your findings.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boot.fn <- function(data, index) {
    return(quantile(data$medv[index], c(0.1)))
  }
  boot(boston, boot.fn, R=1000)
#+END_SRC 

#+RESULTS:
#+begin_example

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = boston, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
    original   bias    std. error
t1*    12.75 -0.01065   0.4976814
#+end_example

Again, the results are the same with a low standard error.

