#+TITLE: Introduction to Statistical Learning, Chapter 2: Overview of Statistical Learning
#+AUTHOR: Rupert Lane
#+EMAIL: rupert@rupert-lane.org
#+PROPERTY: header-args:R :session *R*
#+STARTUP: inlineimages
#+STARTUP: latexpreview

* Conceptual
** Question 1
#+BEGIN_QUOTE
For each of parts (a) through (d), indicate whether we would
generally expect the performance of a flexible statistical learning
method to be better or worse than an inflexible method. Justify your
answer.
#+END_QUOTE

#+BEGIN_QUOTE
(a) The sample size /n/ is extremely large, and the number of
predictors /p/ is small.
#+END_QUOTE

Flexible would be better, as large /n/ and low /p/ reduces the chance of
fitting the noise in the samples. We can benefit from the low bias of
a flexible model without too much variance.

#+BEGIN_QUOTE
(b) The number of predictors /p/ is extremely large, and the number
of observations /n/ is small.
#+END_QUOTE

Flexible would be worse; this is the opposite case to a). We'd likely
see much more variance from the flexible model due to the high chance
of changes for other data sets given low /n/, and the curse of
dimensionality from high /p/.

#+BEGIN_QUOTE
(c) The relationship between the predictors and response is highly
non-linear.
#+END_QUOTE

Flexible would be better, as the non-linear nature of the relationship
implies we need a more complex model. A less flexible model would have
worse bias.

#+BEGIN_QUOTE
(d) The variance of the error terms, i.e. /σ² = Var(ε)/, is extremely
high.
#+END_QUOTE

Flexible would be worse, as it is likely to over-fit the noise -
variance would dominate the errors.
** Question 2
#+BEGIN_QUOTE
Explain whether each scenario is a classiﬁcation or regression
problem, and indicate whether we are most interested in inference or
prediction. Finally, provide /n/ and /p/.
#+END_QUOTE

#+BEGIN_QUOTE
(a) We collect a set of data on the top 500 ﬁrms in the US. For each
ﬁrm we record proﬁt, number of employees, industry and the CEO salary.
We are interested in understanding which factors affect CEO salary.
#+END_QUOTE

- Regression: the response variable, salary, is known and quantitative.
- Inference: we want to know what affects the salary.
- /n/ = 500, /p/ = 3

#+BEGIN_QUOTE
(b) We are considering launching a new product and wish to know
whether it will be a success or a failure. We collect data on 20
similar products that were previously launched. For each product we
have recorded whether it was a success or failure, price charged for
the product, marketing budget, competition price, and ten other
variables.
#+END_QUOTE

- Classification: success or failure of product is qualitative.
- Prediction: we want to know the outcome rather than why.
- /n/ = 20, /p/ = 13

#+BEGIN_QUOTE
(c) We are interest ed in predicting the % change in the USD/Euro
exchange rate in relation to the weekly changes in the world
stock markets. Hence we collect weekly data for all of 2012. For
each week we record the % change in the USD/Euro, the %
change in the US market, the % change in the British market,
and the % change in the German market.
#+END_QUOTE

- Regression, as we want to predict the change in an exchange rate
  which is quantitative.
- Prediction as we are less interested in why this happens than
  predicting the rate.
- /n/ = 52, /p/ = 3
** Question 3
#+BEGIN_QUOTE
We now revisit the bias-variance decomposition.
#+END_QUOTE

#+BEGIN_QUOTE
(a) Provide a sketch of typical (squared) bias, variance, training
error, test error, and Bayes (or irreducible) error curves, on a
single plot, as we go from less flexible statistical learning methods
towards more flexible approaches. The x-axis should represent the
amount of flexibility in the method, and the y-axis should represent
the values for each curve. There should be ﬁve curves. Make sure to
label each one.
#+END_QUOTE

A very rough plot:

[[file:img/ch02q03a.png][Q3 plot]]

#+BEGIN_QUOTE
(b) Explain why each of the ﬁve curves has the shape displayed in
part (a).
#+END_QUOTE

- The (squared) bias gives the error from the lack of capability of
  the model, so will be high for low flexibility models and go down.
- The variance shows the effect of different data sets on the same
  model, so will be low for low complexity models which are less
  susceptible to noise and go up with more flexibility.
- Training error will go down as the model gets more flexible, as we
  fit and more and more to the data.
- But test error will start high, get better and then get worse -
  based on the effects of combining bias and variance.
- Finally, Bayes error or irreducible error is inherent in the data
  (eg measurement error) and will be constant.
** Question 4
#+BEGIN_QUOTE
You will now think of some real-life applications for statistical
learning.
#+END_QUOTE

#+BEGIN_QUOTE
(a) Describe three real-life applications in which classification might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your answer.
#+END_QUOTE

- Given a piece of text, determine what language it is. Response is
  language, predictors may be characters used, frequency, words used
  and probably many others. Goal is prediction.
- Is an incoming website request malicious or not? Response is yes/no
  (or maybe a probability) and predictors may include IP address, HTTP
  headers, cookies etc. Goal is also prediction.
- What determines the weather? Response could be sunny, raining etc.
  Predictors could be pressure, temperature, time of year etc.
  Application could be inference, to understand why, or prediction.

#+BEGIN_QUOTE
(b) Describe three real-life applications in which regression might be
useful. Describe the response, as well as the predictors. Is the goal
of each application inference or prediction? Explain your answer.
#+END_QUOTE

- Does the height of a child depend on the heights of the father or
  the mother? Response is height of the child, inputs are the heights
  of the two parents, and this is inference.
- What factors cause flight delays? Response is delay, predictors
  could be weather, departure time, airport size etc. Inferential.
- What yield will we get from a crop? Response is amount of crop,
  predictors could be planting date, soil quality, amount of chemicals
  used etc. Predictive.

#+BEGIN_QUOTE
(c) Describe three real-life applications in which cluster analysis
might be useful.
#+END_QUOTE

- What segment of Netflix users like this program?
- Grouping sets of search engine results by topic.
- Analysing what objects are in an image.
** Question 5
#+BEGIN_QUOTE
What are the advantages and disadvantages of a very flexible (versus a
less flexible) approach for regression or classiﬁcation? Under what
circumstances might a more flexible approach be preferred to a less
flexible approach? When might a less flexible approach be preferred?
#+END_QUOTE

A flexible model can model more complex data patterns, such as a
non-linear relationship (low bias). However, it does not deal so well
with noise (high variance) and it's more complex to infer why the
response variable behaves the way it does.
** Question 6
#+BEGIN_QUOTE
Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric
approach to regression or classiﬁcation (as opposed to a non-
parametric approach)? What are its disadvantages?
#+END_QUOTE

In the parametric approach you choose a model and fit to the data,
whereas the non-parametric approach does not make assumptions about
what model is best. The non-parametric approach can thus be more
flexible, but there is a risk of over-fitting the training data.
Generally you need more observations for the non-parametric approach.
** Question 7
#+BEGIN_QUOTE
The table below provides a training data set containing six
observations, three predictors, and one qualitative response variable.

| Obs. | X1 | X2 | X3 | Y     |
|------+----+----+----+-------|
|    1 |  0 |  3 |  0 | Red   |
|    2 |  2 |  0 |  0 | Red   |
|    3 |  0 |  1 |  3 | Red   |
|    4 |  0 |  1 |  2 | Green |
|    5 | -1 |  0 |  1 | Green |
|    6 |  1 |  1 |  1 | Red   |

Suppose we wish to use this data set to make a prediction for /Y/ when
/X1 = X2 = X3 = 0/ using /K/-nearest neighbours.

(a) Compute the Euclidean distance between each observation and the
test point, /X1 = X2 = X3 = 0/.
#+END_QUOTE

The Euclidean distance is the square root of the sum of squared
distances in each dimension.

| Obs. | Distance |
|------+----------|
|    0 |        3 |
|    1 |        2 |
|    2 |    3.162 |
|    4 |    2.236 |
|    5 |    1.414 |
|    6 |    1.732 |

#+BEGIN_QUOTE
(b) What is our prediction with /K/ = 1? Why?
#+END_QUOTE

Green, as the nearest neighbour with /K/ = 1 is observation 5.

#+BEGIN_QUOTE
(c) What is our prediction with /K/ = 3? Why?
#+END_QUOTE

The closest three points are 5, 6 and 2. The prediction is therefore
red as two of these three points are red.

#+BEGIN_QUOTE
(d) If the Bayes decision boundary in this problem is highly non-
linear, then would we expect the best value for /K/ to be large or
small? Why?
#+END_QUOTE

Small, as high values of K lead to a smoother boundary.
* Applied
Note we will be using [[https://www.tidyverse.org/][Tidyverse]] functions instead of plain R.

#+BEGIN_SRC R :exports code :results none
library(ISLR)
library(tidyverse)
library(ggplot2)
library(GGally)
library(gridExtra)

options(crayon.enabled = FALSE)
#+END_SRC
** Question 8
#+BEGIN_QUOTE
This exercise relates to the ~College~ data set, which can be found in
the ﬁle ~College.csv~.
#+END_QUOTE

#+BEGIN_QUOTE
(a) Use the ~read.csv()~ function to read the data into R . Call the
loaded data ~college~ . Make sure that you have the directory set to the
correct location for the data.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  ## We use read_csv from readr instead
  college <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/College.csv")
#+END_SRC 

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  X1 = col_character(),
  Private = col_character(),
  Apps = col_double(),
  Accept = col_double(),
  Enroll = col_double(),
  Top10perc = col_double(),
  Top25perc = col_double(),
  F.Undergrad = col_double(),
  P.Undergrad = col_double(),
  Outstate = col_double(),
  Room.Board = col_double(),
  Books = col_double(),
  Personal = col_double(),
  PhD = col_double(),
  Terminal = col_double(),
  S.F.Ratio = col_double(),
  perc.alumni = col_double(),
  Expend = col_double(),
  Grad.Rate = col_double()
)
Warning message:
Missing column names filled in: 'X1' [1]
#+end_example

Note that the first column, which is unnamed in the CSV file, has been
called X1. 

#+BEGIN_QUOTE
(b) Look at the data using the ~fix()~ function. You should notice
that the ﬁrst column is just the name of each university. We don’t
really want R to treat this as data. However, it may be handy to have
these names for later.
#+END_QUOTE

As row names are depreciated for tibbles, we'll just rename the
column.

#+BEGIN_SRC R :results output :exports both
  college <- rename(college, name = X1)
  glimpse(college)
#+END_SRC 

#+RESULTS:
#+begin_example

Observations: 777
Variables: 19
$ name        <chr> "Abilene Christian University", "Adelphi University", "...
$ Private     <chr> "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes",...
$ Apps        <dbl> 1660, 2186, 1428, 417, 193, 587, 353, 1899, 1038, 582, ...
$ Accept      <dbl> 1232, 1924, 1097, 349, 146, 479, 340, 1720, 839, 498, 1...
$ Enroll      <dbl> 721, 512, 336, 137, 55, 158, 103, 489, 227, 172, 472, 4...
$ Top10perc   <dbl> 23, 16, 22, 60, 16, 38, 17, 37, 30, 21, 37, 44, 38, 44,...
$ Top25perc   <dbl> 52, 29, 50, 89, 44, 62, 45, 68, 63, 44, 75, 77, 64, 73,...
$ F.Undergrad <dbl> 2885, 2683, 1036, 510, 249, 678, 416, 1594, 973, 799, 1...
$ P.Undergrad <dbl> 537, 1227, 99, 63, 869, 41, 230, 32, 306, 78, 110, 44, ...
$ Outstate    <dbl> 7440, 12280, 11250, 12960, 7560, 13500, 13290, 13868, 1...
$ Room.Board  <dbl> 3300, 6450, 3750, 5450, 4120, 3335, 5720, 4826, 4400, 3...
$ Books       <dbl> 450, 750, 400, 450, 800, 500, 500, 450, 300, 660, 500, ...
$ Personal    <dbl> 2200, 1500, 1165, 875, 1500, 675, 1500, 850, 500, 1800,...
$ PhD         <dbl> 70, 29, 53, 92, 76, 67, 90, 89, 79, 40, 82, 73, 60, 79,...
$ Terminal    <dbl> 78, 30, 66, 97, 72, 73, 93, 100, 84, 41, 88, 91, 84, 87...
$ S.F.Ratio   <dbl> 18.1, 12.2, 12.9, 7.7, 11.9, 9.4, 11.5, 13.7, 11.3, 11....
$ perc.alumni <dbl> 12, 16, 30, 37, 2, 11, 26, 37, 23, 15, 31, 41, 21, 32, ...
$ Expend      <dbl> 7041, 10527, 8735, 19016, 10922, 9727, 8861, 11487, 116...
$ Grad.Rate   <dbl> 60, 56, 54, 59, 15, 55, 63, 73, 80, 52, 73, 76, 74, 68,...
#+end_example

#+BEGIN_QUOTE
(c) i. Use the ~summary()~ function to produce a numerical summary of
the variables in the data set.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  summary(college)
#+END_SRC 

#+RESULTS:
#+begin_example
     name             Private               Apps           Accept     
 Length:777         Length:777         Min.   :   81   Min.   :   72  
 Class :character   Class :character   1st Qu.:  776   1st Qu.:  604  
 Mode  :character   Mode  :character   Median : 1558   Median : 1110  
                                       Mean   : 3002   Mean   : 2019  
                                       3rd Qu.: 3624   3rd Qu.: 2424  
                                       Max.   :48094   Max.   :26330  
     Enroll       Top10perc       Top25perc      F.Undergrad   
 Min.   :  35   Min.   : 1.00   Min.   :  9.0   Min.   :  139  
 1st Qu.: 242   1st Qu.:15.00   1st Qu.: 41.0   1st Qu.:  992  
 Median : 434   Median :23.00   Median : 54.0   Median : 1707  
 Mean   : 780   Mean   :27.56   Mean   : 55.8   Mean   : 3700  
 3rd Qu.: 902   3rd Qu.:35.00   3rd Qu.: 69.0   3rd Qu.: 4005  
 Max.   :6392   Max.   :96.00   Max.   :100.0   Max.   :31643  
  P.Undergrad         Outstate       Room.Board       Books       
 Min.   :    1.0   Min.   : 2340   Min.   :1780   Min.   :  96.0  
 1st Qu.:   95.0   1st Qu.: 7320   1st Qu.:3597   1st Qu.: 470.0  
 Median :  353.0   Median : 9990   Median :4200   Median : 500.0  
 Mean   :  855.3   Mean   :10441   Mean   :4358   Mean   : 549.4  
 3rd Qu.:  967.0   3rd Qu.:12925   3rd Qu.:5050   3rd Qu.: 600.0  
 Max.   :21836.0   Max.   :21700   Max.   :8124   Max.   :2340.0  
    Personal         PhD            Terminal       S.F.Ratio    
 Min.   : 250   Min.   :  8.00   Min.   : 24.0   Min.   : 2.50  
 1st Qu.: 850   1st Qu.: 62.00   1st Qu.: 71.0   1st Qu.:11.50  
 Median :1200   Median : 75.00   Median : 82.0   Median :13.60  
 Mean   :1341   Mean   : 72.66   Mean   : 79.7   Mean   :14.09  
 3rd Qu.:1700   3rd Qu.: 85.00   3rd Qu.: 92.0   3rd Qu.:16.50  
 Max.   :6800   Max.   :103.00   Max.   :100.0   Max.   :39.80  
  perc.alumni        Expend        Grad.Rate     
 Min.   : 0.00   Min.   : 3186   Min.   : 10.00  
 1st Qu.:13.00   1st Qu.: 6751   1st Qu.: 53.00  
 Median :21.00   Median : 8377   Median : 65.00  
 Mean   :22.74   Mean   : 9660   Mean   : 65.46  
 3rd Qu.:31.00   3rd Qu.:10830   3rd Qu.: 78.00  
 Max.   :64.00   Max.   :56233   Max.   :118.00
#+end_example

#+BEGIN_QUOTE
(c) ii. Use the ~pairs()~ function to produce a scatterplot matrix of
the ﬁrst ten columns or variables of the data.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics  :file img/ch02q08cii.png :width 800 :height 800
  ggpairs(college[,2:11], progress=FALSE)
#+END_SRC

#+RESULTS:
[[file:img/ch02q08cii.png]]

#+BEGIN_QUOTE
(c) iii. Use the ~plot()~ function to produce side-by-side boxplots of
~Outstate~ versus ~Private~.
#+END_QUOTE

See the below code. This can also be seen on the top row of the plot
for ii.

#+BEGIN_SRC R :exports both :results graphics  :file img/ch02q08ciii.png :width 600
  ggplot(college, aes(x=Private, y=Outstate)) + 
      geom_boxplot() +
      labs(title = "Out-of-state tuition box plot for private/public universities")
#+END_SRC

#+RESULTS:
[[file:img/ch02q08ciii.png]]

#+BEGIN_QUOTE
(c) iv. Create a new qualitative variable, called ~Elite~ , by binning
the ~Top10perc~ variable. We are going to divide universities into two
groups based on whether or not the proportion of students coming from
the top 10% of their high school classes exceeds 50%.

~Elite = rep(" No " , nrow(college))~
~Elite [ college$Top10perc >50]="Yes"~
~Elite = as.factor(Elite)~
~college = data.frame(college, Elite)~

Use the ~summary()~ function to see how many elite universities there
are. Now use the ~plot()~ function to produce side-by-side boxplots of
~Outstate~ versus ~Elite~.
#+END_QUOTE

#+BEGIN_SRC R  :exports both
  college = mutate(college, Elite = ifelse(Top10perc > 50, "Yes", "No"))
  college %>% count(Elite)
#+END_SRC 

#+RESULTS:
| No  | 699 |
| Yes |  78 |

#+BEGIN_SRC R :exports both :results graphics :file img/ch02q08civ.png :width 600
  ggplot(college, aes(x=Elite, y=Outstate)) + 
      geom_boxplot() +
      labs(title = "Out-of-state tuition box plot for elite (top 10%) students")
#+END_SRC

#+RESULTS:
[[file:img/ch02q08civ.png]]

#+BEGIN_QUOTE
(c) v. Use the ~hist()~ function to produce some histograms with
differing numbers of bins for a few of the quantitative variables. You
may ﬁnd the command ~par(mfrow=c(2,2))~ useful: it will divide the
print window into four regions so that four plots can be made
simultaneously. Modifying the arguments to this function will divide
the screen in other ways.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics :file img/ch02q08cv.png :width 800
  widths <- c(100, 250, 500, 1000)
  pl <- lapply(widths, function(.x) ggplot(college, aes(Outstate)) +
                                    geom_histogram(binwidth=.x) +
                                    labs(title=paste("Bin width =", .x)))
  marrangeGrob(pl, nrow=2, ncol=2,
               top="Out-of-state tuition with various bin widths")
#+END_SRC

#+RESULTS:
[[file:img/ch02q08cv.png]]

#+BEGIN_QUOTE
(c) vi. Continue exploring the data, and provide a brief summary of
what you discover.
#+END_QUOTE

One question I wanted to answer is how many applications turn into
enrolments?

#+BEGIN_SRC R :results output :exports both
  college <- mutate(college, EnrollPerc = Enroll / Apps)
  college %>% select(EnrollPerc) %>% summary
  collegeByEnrollPerc <- college %>% arrange(EnrollPerc) %>% select(name, EnrollPerc) 
  print("Top 5 enrolment")
  top_n(collegeByEnrollPerc, -5) 
  print("Bottom 5 enrolment")
  top_n(collegeByEnrollPerc, 5) %>% arrange(desc(EnrollPerc))
#+END_SRC 

#+RESULTS:
#+begin_example

   EnrollPerc     
 Min.   :0.06892  
 1st Qu.:0.22011  
 Median :0.29152  
 Mean   :0.30937  
 3rd Qu.:0.38268  
 Max.   :0.83705

[1] "Top 5 enrolment"

Selecting by EnrollPerc
# A tibble: 5 x 2
  name                               EnrollPerc
  <chr>                                   <dbl>
1 Rutgers State University at Camden     0.0689
2 Talladega College                      0.0759
3 SUNY College at New Paltz              0.0781
4 Franklin Pierce College                0.0860
5 Rutgers State University at Newark     0.0863

[1] "Bottom 5 enrolment"

Selecting by EnrollPerc
# A tibble: 5 x 2
  name                                    EnrollPerc
  <chr>                                        <dbl>
1 Brewton-Parker College                       0.837
2 Mississippi University for Women             0.792
3 Hardin-Simmons University                    0.749
4 Dickinson State University                   0.735
5 University of Sci. and Arts of Oklahoma      0.730
#+end_example

#+BEGIN_SRC R :exports both :results graphics  :file img/ch02q08cvi.png :width 600 :height 300
  ggplot(college, aes(y=EnrollPerc)) + 
      geom_boxplot() +
      coord_flip() + 
      labs(title = "Enroll % box plot")
#+END_SRC

#+RESULTS:
[[file:img/ch02q08cvi.png]]

** Question 9
#+BEGIN_QUOTE
This exercise involves the ~Auto~ data set studied in the lab. Make sure
that the missing values have been removed from the data.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  auto <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Auto.csv", 
                   na = "?") %>% na.omit()
  glimpse(auto)
#+END_SRC 

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  mpg = col_double(),
  cylinders = col_double(),
  displacement = col_double(),
  horsepower = col_double(),
  weight = col_double(),
  acceleration = col_double(),
  year = col_double(),
  origin = col_double(),
  name = col_character()
)

Observations: 392
Variables: 9
$ mpg          <dbl> 18, 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14...
$ cylinders    <dbl> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, ...
$ displacement <dbl> 307, 350, 318, 304, 302, 429, 454, 440, 455, 390, 383,...
$ horsepower   <dbl> 130, 165, 150, 150, 140, 198, 220, 215, 225, 190, 170,...
$ weight       <dbl> 3504, 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, ...
$ acceleration <dbl> 12.0, 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8....
$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70...
$ origin       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, ...
$ name         <chr> "chevrolet chevelle malibu", "buick skylark 320", "ply...
#+end_example

#+BEGIN_QUOTE
(a) Which of the predictors are quantitative, and which are
qualitative?
#+END_QUOTE

~name~ is qualitative. Not sure what ~origin~ represents so let's take
a look:

#+BEGIN_SRC R :results output :exports both
  table(auto$origin)
#+END_SRC 

#+RESULTS:
: 
:   1   2   3 
: 245  68  79
 
Looks like some kind of category, so say it's qualitative. The other
variables are all quantitative.

#+BEGIN_QUOTE
(b) What is the range of each quantitative predictor? You can answer
this using the ~range()~ function.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  autoQuant <- auto[,1:7]
  summarise_all(autoQuant, min)
  summarise_all(autoQuant, max)
#+END_SRC 

#+RESULTS:
#+begin_example

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1     9         3           68         46   1613            8    70

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  46.6         8          455        230   5140         24.8    82
#+end_example

#+BEGIN_QUOTE
(c) What is the mean and standard deviation of each quantitative
predictor?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  autoQuant <- auto[,1:7]
  summarise_all(autoQuant, mean)
  summarise_all(autoQuant, sd)
#+END_SRC 

#+RESULTS:
#+begin_example

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  23.4      5.47         194.       104.  2978.         15.5  76.0

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  7.81      1.71         105.       38.5   849.         2.76  3.68
#+end_example

#+BEGIN_QUOTE
(d) Now remove the 10th through 85th observations. What is the
range, mean, and standard deviation of each predictor in the
subset of the data that remains?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  autoSubset <- filter(autoQuant, row_number() < 10 | row_number() > 85)
  summarise_all(autoSubset, min)
  summarise_all(autoSubset, max)
  summarise_all(autoSubset, mean)
  summarise_all(autoSubset, sd)
#+END_SRC 

#+RESULTS:
#+begin_example

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1    11         3           68         46   1649          8.5    70

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  46.6         8          455        230   4997         24.8    82

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  24.4      5.37         187.       101.  2936.         15.7  77.1

# A tibble: 1 x 7
    mpg cylinders displacement horsepower weight acceleration  year
  <dbl>     <dbl>        <dbl>      <dbl>  <dbl>        <dbl> <dbl>
1  7.87      1.65         99.7       35.7   811.         2.69  3.11
#+end_example

#+BEGIN_QUOTE
(e) Using the full data set, investigate the predictors graphically,
using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your ﬁndings.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics  :file img/ch02q09e.png :width 800 :height 800
  ggpairs(autoQuant, progress=FALSE)
#+END_SRC

#+RESULTS:
[[file:img/ch02q09e.png]]

A number of relationships can be seen there, such as horsepower vs
weight or displacement.

#+BEGIN_QUOTE
(f) Suppose that we wish to predict gas mileage (~mpg~) on the basis
of the other variables. Do your plots suggest that any of the
other variables might be useful in predicting ~mpg~? Justify your
answer.
#+END_QUOTE

There is a non-linear relationship between mpg and horsepower, weight
and displacement. There is a weak (correlation = 0.429) relationship
with acceleration.
** Question 10
#+BEGIN_QUOTE
This exercise involves the ~Boston~ housing data set.
#+END_QUOTE

#+BEGIN_QUOTE
(a) To begin, load in the ~Boston~ data set. The Boston data set is
part of the MASS library in R .

~> library(MASS)~

Now the data set is contained in the object ~Boston~.

~> Boston

Read about the data set:

~> ? Boston~

How many rows are in this data set? How many columns? What do the rows
and columns represent?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  library(MASS)
  glimpse(Boston)
#+END_SRC 

#+RESULTS:
#+begin_example

Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

Observations: 506
Variables: 14
$ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.088...
$ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5...
$ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87,...
$ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
$ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.5...
$ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.6...
$ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9...
$ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9...
$ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4,...
$ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311,...
$ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2,...
$ black   <dbl> 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396...
$ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17...
$ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9,...
#+end_example

506 rows and 14 columns. The data represents housing values in suburbs
of Boston. Each row represents a town or tract and has variables like
crime per capita, proportion of non-retail business acres per town,
tax rate etc.

#+BEGIN_QUOTE
(b) Make some pairwise scatterplots of the predictors (columns) in
this data set. Describe your ﬁndings.
#+END_QUOTE

#+BEGIN_SRC R :exports both :results graphics  :file img/ch02q10b.png :width 800 :height 800
  ggpairs(Boston, progress=FALSE)
#+END_SRC

#+RESULTS:
[[file:img/ch02q10b.png]]

There are a number of relationships that look interesting, includinh

- ~nox~ (nitrogen oxide concentration) vs ~dis~ (distance to employment)
- ~rm~ (rooms per dwelling) vs ~medv~ (median value of dwellings)

Looking at distributions (the histograms) is interesting, with
several variables (~rad~, ~tax~, ~indus~) having two peaks.

#+BEGIN_QUOTE
(c) Are any of the predictors associated with per capita crime rate?
If so, explain the relationship.
#+END_QUOTE

Looking at correlations:

#+BEGIN_SRC R :results output :exports both
  cor(Boston$crim, Boston)
#+END_SRC 

#+RESULTS:
:      crim         zn     indus        chas       nox         rm       age
: [1,]    1 -0.2004692 0.4065834 -0.05589158 0.4209717 -0.2192467 0.3527343
:             dis       rad       tax   ptratio      black     lstat       medv
: [1,] -0.3796701 0.6255051 0.5827643 0.2899456 -0.3850639 0.4556215 -0.3883046

We can see the strongest correlations for ~rad~, ~tax~ and ~lstat~.

- ~rad~ at its highest value is associated with more crime on average,
  and a greater range. At other points there is not much association.
- ~tax~ is similar to ~rad~.
- Crime goes up with increased ~lstat~.

#+BEGIN_QUOTE
(d) Do any of the suburbs of Boston appear to have particularly high
crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of
each predictor.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  # select was masked by Boston
  bostonSubset <- dplyr::select(Boston, crim, tax, ptratio)
  summary(bostonSubset)
#+END_SRC 

#+RESULTS:
: 
:       crim               tax           ptratio     
:  Min.   : 0.00632   Min.   :187.0   Min.   :12.60  
:  1st Qu.: 0.08204   1st Qu.:279.0   1st Qu.:17.40  
:  Median : 0.25651   Median :330.0   Median :19.05  
:  Mean   : 3.61352   Mean   :408.2   Mean   :18.46  
:  3rd Qu.: 3.67708   3rd Qu.:666.0   3rd Qu.:20.20  
:  Max.   :88.97620   Max.   :711.0   Max.   :22.00

With this and the histograms we can see
- Crime is generally low, but there are some very large outliers
- Tax rates have two peaks
- Pupil/teacher ratio is negatively skewed

#+BEGIN_QUOTE
(e) How many of the suburbs in this data set bound the Charles river?
#+END_QUOTE

The ~chas~ variable shows this, if the value is 1 it is bound by the river.

#+BEGIN_SRC R :results output :exports both
  table(Boston$chas)
#+END_SRC 

#+RESULTS:
: 
:   0   1 
: 471  35

#+BEGIN_QUOTE
(f) What is the median pupil-teacher ratio among the towns in this
data set?
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  median(Boston$ptratio)
#+END_SRC 

#+RESULTS:
: [1] 19.05

#+BEGIN_QUOTE
(g) Which suburb of Boston has lowest median value of owner-occupied
homes? What are the values of the other predictors for that suburb,
and how do those values compare to the overall ranges for those
predictors? Comment on your ﬁndings.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  summary(Boston$medv)
  bostonLowMedv <- filter(Boston, medv < 10) 
  bostonLowMedv %>% group_by(zn, chas) %>% tally()
  bostonLowMedv %>% arrange(medv) %>% dplyr::select(-chas, -zn)
#+END_SRC 

#+RESULTS:
#+begin_example
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   5.00   17.02   21.20   22.53   25.00   50.00

# A tibble: 1 x 3
# Groups:   zn [?]
     zn  chas     n
  <dbl> <int> <int>
1     0     0    24

       crim indus   nox    rm   age    dis rad tax ptratio  black lstat medv
1  38.35180 18.10 0.693 5.453 100.0 1.4896  24 666    20.2 396.90 30.59  5.0
2  67.92080 18.10 0.693 5.683 100.0 1.4254  24 666    20.2 384.97 22.98  5.0
3  25.04610 18.10 0.693 5.987 100.0 1.5888  24 666    20.2 396.90 26.77  5.6
4   9.91655 18.10 0.693 5.852  77.8 1.5004  24 666    20.2 338.16 29.97  6.3
5  45.74610 18.10 0.693 4.519 100.0 1.6582  24 666    20.2  88.27 36.98  7.0
6   0.18337 27.74 0.609 5.414  98.3 1.7554   4 711    20.1 344.05 23.97  7.0
7  16.81180 18.10 0.700 5.277  98.1 1.4261  24 666    20.2 396.90 30.81  7.2
8  14.23620 18.10 0.693 6.343 100.0 1.5741  24 666    20.2 396.90 20.32  7.2
9  18.08460 18.10 0.679 6.434 100.0 1.8347  24 666    20.2  27.25 29.05  7.2
10 22.59710 18.10 0.700 5.000  89.5 1.5184  24 666    20.2 396.90 31.99  7.4
11 10.83420 18.10 0.679 6.782  90.8 1.8195  24 666    20.2  21.57 25.79  7.5
12  0.20746 27.74 0.609 5.093  98.0 1.8226   4 711    20.1 318.43 29.68  8.1
13 24.80170 18.10 0.693 5.349  96.0 1.7028  24 666    20.2 396.90 19.77  8.3
14 15.86030 18.10 0.679 5.896  95.4 1.9096  24 666    20.2   7.68 24.39  8.3
15 11.81230 18.10 0.718 6.824  76.5 1.7940  24 666    20.2  48.45 22.74  8.4
16 13.67810 18.10 0.740 5.935  87.9 1.8206  24 666    20.2  68.95 34.02  8.4
17  7.67202 18.10 0.693 5.747  98.9 1.6334  24 666    20.2 393.10 19.92  8.5
18 41.52920 18.10 0.693 5.531  85.4 1.6074  24 666    20.2 329.46 27.38  8.5
19 15.17720 18.10 0.740 6.152 100.0 1.9142  24 666    20.2   9.32 26.45  8.7
20 20.08490 18.10 0.700 4.368  91.2 1.4395  24 666    20.2 285.83 30.63  8.8
21 73.53410 18.10 0.679 5.957 100.0 1.8026  24 666    20.2  16.45 20.62  8.8
22  9.33889 18.10 0.679 6.380  95.6 1.9682  24 666    20.2  60.72 24.08  9.5
23 14.42080 18.10 0.740 6.461  93.3 2.0026  24 666    20.2  27.49 18.05  9.6
24 11.57790 18.10 0.700 5.036  97.0 1.7700  24 666    20.2 396.90 25.68  9.7
#+end_example

- None of these have large residential plots or are by the Charles river
- Crime rates are generally high
- Industrial zoning is above the mean
- Nitrogen oxide rates are all above the mean
- Age is well above the mean
- Tax and lower status is high

#+BEGIN_QUOTE
(h) In this data set, how many of the suburbs average more than seven
rooms per dwelling? More than eight rooms per dwelling? Comment on the
suburbs that average more than eight rooms per dwelling.
#+END_QUOTE

#+BEGIN_SRC R :results output :exports both
  boston7Rooms <- filter(Boston, rm > 7) 
  count(boston7Rooms)
  boston8Rooms <- filter(Boston, rm > 8) 
  count(boston8Rooms)
  boston8Rooms %>% arrange(medv)  %>% dplyr::select(-rm)
#+END_SRC 

#+RESULTS:
#+begin_example

# A tibble: 1 x 1
      n
  <int>
1    64

# A tibble: 1 x 1
      n
  <int>
1    13

      crim zn indus chas    nox  age    dis rad tax ptratio  black lstat medv
1  3.47428  0 18.10    1 0.7180 82.9 1.9047  24 666    20.2 354.55  5.29 21.9
2  0.38214  0  6.20    0 0.5040 86.5 3.2157   8 307    17.4 387.38  3.13 37.6
3  0.12083  0  2.89    0 0.4450 76.0 3.4952   2 276    18.0 396.90  4.21 38.7
4  0.57529  0  6.20    0 0.5070 73.3 3.8384   8 307    17.4 385.91  2.47 41.7
5  0.36894 22  5.86    0 0.4310  8.4 8.9067   7 330    19.1 396.90  3.54 42.8
6  0.31533  0  6.20    0 0.5040 78.3 2.8944   8 307    17.4 385.05  4.14 44.8
7  0.33147  0  6.20    0 0.5070 70.4 3.6519   8 307    17.4 378.95  3.95 48.3
8  0.52014 20  3.97    0 0.6470 91.5 2.2885   5 264    13.0 386.86  5.91 48.8
9  1.51902  0 19.58    1 0.6050 93.9 2.1620   5 403    14.7 388.45  3.32 50.0
10 0.02009 95  2.68    0 0.4161 31.9 5.1180   4 224    14.7 390.55  2.88 50.0
11 0.52693  0  6.20    0 0.5040 83.0 2.8944   8 307    17.4 382.00  4.63 50.0
12 0.61154 20  3.97    0 0.6470 86.9 1.8010   5 264    13.0 389.70  5.12 50.0
13 0.57834 20  3.97    0 0.5750 67.0 2.4216   5 264    13.0 384.54  7.44 50.0
#+end_example

- Crime is below the mean
- Age is generally high, with one exception
- Close to radial highways, with one exception.
- Lower status is low
- Median value is high


